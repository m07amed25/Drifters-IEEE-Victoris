{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90122e09"
      },
      "source": [
        "# Task\n",
        "Adapt the \"https://huggingface.co/OPear/videomae-large-finetuned-UCF-Crime\" model for anomaly detection in CCTV footage, given that you only have examples of normal behavior and need to identify deviations from this normal behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdfbc6f4"
      },
      "source": [
        "## Load the pre-trained model\n",
        "\n",
        "### Subtask:\n",
        "Load the `OPear/videomae-large-finetuned-UCF-Crime` model from Hugging Face.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "385c63e0"
      },
      "outputs": [],
      "source": [
        "from transformers import VivitForVideoClassification, AutoImageProcessor\n",
        "\n",
        "model_name = \"prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "model = VivitForVideoClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93753395"
      },
      "source": [
        "Let's inspect the model's configuration and layers to understand its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e53feaad",
        "outputId": "99306846-9a28-46a1-dcc2-9fc89ffe8feb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Configuration:\n",
            "VivitConfig {\n",
            "  \"architectures\": [\n",
            "    \"VivitForVideoClassification\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"hidden_act\": \"gelu_fast\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"Abuse\",\n",
            "    \"1\": \"Arrest\",\n",
            "    \"2\": \"Arson\",\n",
            "    \"3\": \"Assault\",\n",
            "    \"4\": \"Burglary\",\n",
            "    \"5\": \"Explosion\",\n",
            "    \"6\": \"Fighting\",\n",
            "    \"7\": \"Normal\",\n",
            "    \"8\": \"RoadAccidents\",\n",
            "    \"9\": \"Robbery\",\n",
            "    \"10\": \"Shooting\",\n",
            "    \"11\": \"Shoplifting\",\n",
            "    \"12\": \"Stealing\",\n",
            "    \"13\": \"Vandalism\"\n",
            "  },\n",
            "  \"image_size\": 224,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"label2id\": {\n",
            "    \"Abuse\": 0,\n",
            "    \"Arrest\": 1,\n",
            "    \"Arson\": 2,\n",
            "    \"Assault\": 3,\n",
            "    \"Burglary\": 4,\n",
            "    \"Explosion\": 5,\n",
            "    \"Fighting\": 6,\n",
            "    \"Normal\": 7,\n",
            "    \"RoadAccidents\": 8,\n",
            "    \"Robbery\": 9,\n",
            "    \"Shooting\": 10,\n",
            "    \"Shoplifting\": 11,\n",
            "    \"Stealing\": 12,\n",
            "    \"Vandalism\": 13\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-06,\n",
            "  \"model_type\": \"vivit\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_channels\": 3,\n",
            "  \"num_frames\": 32,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.55.4\",\n",
            "  \"tubelet_size\": [\n",
            "    2,\n",
            "    16,\n",
            "    16\n",
            "  ],\n",
            "  \"video_size\": [\n",
            "    32,\n",
            "    224,\n",
            "    224\n",
            "  ]\n",
            "}\n",
            "\n",
            "\n",
            "Model Layers:\n",
            "VivitForVideoClassification(\n",
            "  (vivit): VivitModel(\n",
            "    (embeddings): VivitEmbeddings(\n",
            "      (patch_embeddings): VivitTubeletEmbeddings(\n",
            "        (projection): Conv3d(3, 768, kernel_size=(2, 16, 16), stride=(2, 16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): VivitEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x VivitLayer(\n",
            "          (attention): VivitAttention(\n",
            "            (attention): VivitSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            )\n",
            "            (output): VivitSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): VivitIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "            (intermediate_act_fn): FastGELUActivation()\n",
            "          )\n",
            "          (output): VivitOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=14, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(\"Model Configuration:\")\n",
        "print(model.config)\n",
        "\n",
        "print(\"\\nModel Layers:\")\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a6b2ef0"
      },
      "source": [
        "## Load and preprocess video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77594161",
        "outputId": "88999578-5c61-4800-d583-a0c5740d758d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessed video shape: (32, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "video_path = \"/content/drive/MyDrive/Anomaly_Detection_in_Surveillance_Videos/Anomaly-Videos-Part-1/Abuse/Abuse001_x264.mp4\"\n",
        "target_size = (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n",
        "num_frames_expected = model.config.num_frames\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(f\"Error: Could not open video {video_path}\")\n",
        "else:\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "    while frame_count < num_frames_expected:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        resized_frame = cv2.resize(frame, target_size)\n",
        "        frames.append(resized_frame)\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    if len(frames) < num_frames_expected:\n",
        "        print(f\"Warning: Video has only {len(frames)} frames, expected {num_frames_expected}. Padding with the last frame.\")\n",
        "        while len(frames) < num_frames_expected:\n",
        "            frames.append(frames[-1])\n",
        "\n",
        "\n",
        "    video_array = np.array(frames)\n",
        "\n",
        "    print(f\"Preprocessed video shape: {video_array.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4277e21",
        "outputId": "19398b15-a974-4bf5-e486-a72c1913162a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Video tensor shape after processing: torch.Size([1, 32, 3, 224, 224])\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# The shape is currently (num_frames, height, width, num_channels)\n",
        "# We need (batch_size, num_channels, num_frames, height, width)\n",
        "video_tensor = torch.from_numpy(video_array).permute(3, 0, 1, 2)\n",
        "\n",
        "# batch dimension\n",
        "video_tensor = video_tensor.unsqueeze(0)\n",
        "\n",
        "video_tensor = image_processor(list(torch.unbind(video_tensor.squeeze(0).permute(1, 2, 3, 0))), return_tensors=\"pt\").pixel_values\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "video_tensor = video_tensor.to(device)\n",
        "\n",
        "print(f\"Video tensor shape after processing: {video_tensor.shape}\")\n",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ad26f88",
        "outputId": "020215f2-bfd7-40f2-d6fe-d84237afcd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model outputs obtained.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(video_tensor)\n",
        "\n",
        "print(\"Model outputs obtained.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "363e79c0",
        "outputId": "ef495cf0-96e6-48a4-892e-c39dc1d67423"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted class: Normal\n",
            "Predicted probability: 0.4804\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "probabilities = F.softmax(outputs.logits, dim=1)\n",
        "\n",
        "predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
        "\n",
        "predicted_class_label = model.config.id2label[predicted_class_index]\n",
        "\n",
        "predicted_probability = probabilities[0][predicted_class_index].item()\n",
        "\n",
        "print(f\"Predicted class: {predicted_class_label}\")\n",
        "print(f\"Predicted probability: {predicted_probability:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4eed9f8"
      },
      "source": [
        "# Task\n",
        "Split the preprocessed video data located at \"/content/drive/MyDrive/Preprocessed_Surveillance_Videos\" into training and testing sets, placing each class into its own folder within the respective training and testing directories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6dxBbAU2Wpq",
        "outputId": "8e813f46-58d0-47b3-c097-feafeee588da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Examples of video to class mapping:\n",
            "- Abuse005_x264: Abuse\n",
            "- Abuse007_x264: Abuse\n",
            "- Abuse002_x264: Abuse\n",
            "- Abuse001_x264: Abuse\n",
            "- Abuse003_x264: Abuse\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "preprocessed_output_dir = \"/content/drive/MyDrive/Preprocessed_Surveillance_Videos\"\n",
        "\n",
        "preprocessed_video_dirs = [d for d in os.listdir(preprocessed_output_dir) if os.path.isdir(os.path.join(preprocessed_output_dir, d))]\n",
        "\n",
        "video_class_mapping = {}\n",
        "for video_dir in preprocessed_video_dirs:\n",
        "    match = re.match(r\"([a-zA-Z]+)\", video_dir)\n",
        "    if match:\n",
        "        class_label = match.group(1)\n",
        "        video_class_mapping[video_dir] = class_label\n",
        "    else:\n",
        "        video_class_mapping[video_dir] = \"Unknown\" # Handle cases that don't match the pattern\n",
        "\n",
        "print(\"Examples of video to class mapping:\")\n",
        "for i, (video_dir, class_label) in enumerate(video_class_mapping.items()):\n",
        "    if i >= 5:\n",
        "        break\n",
        "    print(f\"- {video_dir}: {class_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4480fac"
      },
      "source": [
        "## Define split ratios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22a5a9db",
        "outputId": "16b05961-872d-4e21-c4c9-6f6348f28b68"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.8\n",
        "test_ratio = 0.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd267490"
      },
      "source": [
        "## Create directory structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26db0b0a",
        "outputId": "4949f203-e55c-48c3-c111-b992f55e6359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory structure in: /content/drive/MyDrive/Split_Surveillance_Videos\n",
            "Unique classes found: ['Assault', 'Shooting', 'Burglary', 'Arrest', 'Robbery', 'Vandalism', 'Shoplifting', 'Abuse', 'RoadAccidents', 'Explosion', 'Fighting', 'Stealing', 'Arson']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "split_dataset_dir = \"/content/drive/MyDrive/Split_Surveillance_Videos\"\n",
        "train_dir = os.path.join(split_dataset_dir, \"train\")\n",
        "test_dir = os.path.join(split_dataset_dir, \"test\")\n",
        "\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "unique_classes = set(video_class_mapping.values())\n",
        "\n",
        "for class_label in unique_classes:\n",
        "    os.makedirs(os.path.join(train_dir, class_label), exist_ok=True)\n",
        "    os.makedirs(os.path.join(test_dir, class_label), exist_ok=True)\n",
        "\n",
        "print(f\"Created directory structure in: {split_dataset_dir}\")\n",
        "print(f\"Unique classes found: {list(unique_classes)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4beae523"
      },
      "source": [
        "## Split and move data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d5ba25f",
        "outputId": "fe5f0661-df7e-4bca-e1ff-f1ff8e15faa4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing class: Abuse\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Abuse\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Abuse\n",
            "\n",
            "Processing class: Arrest\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Arrest\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Arrest\n",
            "\n",
            "Processing class: Arson\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Arson\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Arson\n",
            "\n",
            "Processing class: Assault\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Assault\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Assault\n",
            "\n",
            "Processing class: Burglary\n",
            "  Total videos: 100\n",
            "  Training videos: 80\n",
            "  Testing videos: 20\n",
            "  Moved 80 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Burglary\n",
            "  Moved 20 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Burglary\n",
            "\n",
            "Processing class: Explosion\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Explosion\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Explosion\n",
            "\n",
            "Processing class: Fighting\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Fighting\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Fighting\n",
            "\n",
            "Processing class: RoadAccidents\n",
            "  Total videos: 150\n",
            "  Training videos: 120\n",
            "  Testing videos: 30\n",
            "  Moved 120 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/RoadAccidents\n",
            "  Moved 30 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/RoadAccidents\n",
            "\n",
            "Processing class: Robbery\n",
            "  Total videos: 150\n",
            "  Training videos: 120\n",
            "  Testing videos: 30\n",
            "  Moved 120 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Robbery\n",
            "  Moved 30 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Robbery\n",
            "\n",
            "Processing class: Shooting\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Shooting\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Shooting\n",
            "\n",
            "Processing class: Shoplifting\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Shoplifting\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Shoplifting\n",
            "\n",
            "Processing class: Stealing\n",
            "  Total videos: 100\n",
            "  Training videos: 80\n",
            "  Testing videos: 20\n",
            "  Moved 80 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Stealing\n",
            "  Moved 20 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Stealing\n",
            "\n",
            "Processing class: Vandalism\n",
            "  Total videos: 50\n",
            "  Training videos: 40\n",
            "  Testing videos: 10\n",
            "  Moved 40 videos to /content/drive/MyDrive/Split_Surveillance_Videos/train/Vandalism\n",
            "  Moved 10 videos to /content/drive/MyDrive/Split_Surveillance_Videos/test/Vandalism\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "# Group video by class\n",
        "class_to_video_dirs = {}\n",
        "for video_dir, class_label in video_class_mapping.items():\n",
        "    if class_label not in class_to_video_dirs:\n",
        "        class_to_video_dirs[class_label] = []\n",
        "    class_to_video_dirs[class_label].append(video_dir)\n",
        "\n",
        "# Split and move videos\n",
        "for class_label, video_dirs in class_to_video_dirs.items():\n",
        "    random.shuffle(video_dirs)\n",
        "\n",
        "    num_train = int(len(video_dirs) * train_ratio)\n",
        "    train_videos = video_dirs[:num_train]\n",
        "    test_videos = video_dirs[num_train:]\n",
        "\n",
        "    print(f\"\\nProcessing class: {class_label}\")\n",
        "    print(f\"  Total videos: {len(video_dirs)}\")\n",
        "    print(f\"  Training videos: {len(train_videos)}\")\n",
        "    print(f\"  Testing videos: {len(test_videos)}\")\n",
        "\n",
        "    # training videos\n",
        "    for video_dir in train_videos:\n",
        "        src_path = os.path.join(preprocessed_output_dir, video_dir)\n",
        "        dest_path = os.path.join(train_dir, class_label, video_dir)\n",
        "        shutil.move(src_path, dest_path)\n",
        "\n",
        "    # testing videos\n",
        "    for video_dir in test_videos:\n",
        "        src_path = os.path.join(preprocessed_output_dir, video_dir)\n",
        "        dest_path = os.path.join(test_dir, class_label, video_dir)\n",
        "        shutil.move(src_path, dest_path)\n",
        "\n",
        "    print(f\"  Moved {len(train_videos)} videos to {os.path.join(train_dir, class_label)}\")\n",
        "    print(f\"  Moved {len(test_videos)} videos to {os.path.join(test_dir, class_label)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca5e60e5"
      },
      "source": [
        "## Verify split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4842be69",
        "outputId": "c87e6745-b76e-4fba-c282-0d5cae42c13b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying data split and movement:\n",
            "\n",
            "Training Set:\n",
            "  Class 'Assault': 40 videos\n",
            "    - Assault033_x264: 256 frames\n",
            "    - Assault035_x264: 180 frames\n",
            "    - Assault036_x264: 183 frames\n",
            "  Class 'Shooting': 40 videos\n",
            "    - Shooting037_x264: 61 frames\n",
            "    - Shooting054_x264: 147 frames\n",
            "    - Shooting014_x264: 1282 frames\n",
            "  Class 'Burglary': 80 videos\n",
            "    - Burglary085_x264: 576 frames\n",
            "    - Burglary070_x264: 160 frames\n",
            "    - Burglary037_x264: 384 frames\n",
            "  Class 'Arrest': 40 videos\n",
            "    - Arrest043_x264: 2160 frames\n",
            "    - Arrest016_x264: 2118 frames\n",
            "    - Arrest031_x264: 370 frames\n",
            "  Class 'Robbery': 120 videos\n",
            "    - Robbery054_x264: 396 frames\n",
            "    - Robbery058_x264: 352 frames\n",
            "    - Robbery027_x264: 1080 frames\n",
            "  Class 'Vandalism': 40 videos\n",
            "    - Vandalism048_x264: 1437 frames\n",
            "    - Vandalism019_x264: 410 frames\n",
            "    - Vandalism018_x264: 1112 frames\n",
            "  Class 'Shoplifting': 40 videos\n",
            "    - Shoplifting014_x264: 13340 frames\n",
            "    - Shoplifting043_x264: 1904 frames\n",
            "    - Shoplifting037_x264: 278 frames\n",
            "  Class 'Abuse': 40 videos\n",
            "    - Abuse015_x264: 1575 frames\n",
            "    - Abuse018_x264: 486 frames\n",
            "    - Abuse031_x264: 889 frames\n",
            "  Class 'RoadAccidents': 120 videos\n",
            "    - RoadAccidents114_x264: 340 frames\n",
            "    - RoadAccidents031_x264: 437 frames\n",
            "    - RoadAccidents139_x264: 728 frames\n",
            "  Class 'Explosion': 40 videos\n",
            "    - Explosion022_x264: 719 frames\n",
            "    - Explosion037_x264: 193 frames\n",
            "    - Explosion044_x264: 144 frames\n",
            "  Class 'Fighting': 40 videos\n",
            "    - Fighting026_x264: 701 frames\n",
            "    - Fighting037_x264: 872 frames\n",
            "    - Fighting020_x264: 442 frames\n",
            "  Class 'Stealing': 80 videos\n",
            "    - Stealing037_x264: 950 frames\n",
            "    - Stealing095_x264: 151 frames\n",
            "    - Stealing043_x264: 963 frames\n",
            "  Class 'Arson': 40 videos\n",
            "    - Arson027_x264: 540 frames\n",
            "    - Arson034_x264: 478 frames\n",
            "    - Arson023_x264: 272 frames\n",
            "\n",
            "Testing Set:\n",
            "  Class 'Assault': 10 videos\n",
            "    - Assault051_x264: 121 frames\n",
            "    - Assault010_x264: 3236 frames\n",
            "    - Assault044_x264: 153 frames\n",
            "  Class 'Shooting': 10 videos\n",
            "    - Shooting017_x264: 189 frames\n",
            "    - Shooting010_x264: 529 frames\n",
            "    - Shooting052_x264: 1114 frames\n",
            "  Class 'Burglary': 20 videos\n",
            "    - Burglary057_x264: 1108 frames\n",
            "    - Burglary092_x264: 125 frames\n",
            "    - Burglary084_x264: 903 frames\n",
            "  Class 'Arrest': 10 videos\n",
            "    - Arrest018_x264: 487 frames\n",
            "    - Arrest038_x264: 439 frames\n",
            "    - Arrest022_x264: 319 frames\n",
            "  Class 'Robbery': 30 videos\n",
            "    - Robbery028_x264: 469 frames\n",
            "    - Robbery129_x264: 449 frames\n",
            "    - Robbery009_x264: 641 frames\n",
            "  Class 'Vandalism': 10 videos\n",
            "    - Vandalism025_x264: 861 frames\n",
            "    - Vandalism044_x264: 921 frames\n",
            "    - Vandalism034_x264: 352 frames\n",
            "  Class 'Shoplifting': 10 videos\n",
            "    - Shoplifting034_x264: 2388 frames\n",
            "    - Shoplifting019_x264: 132 frames\n",
            "    - Shoplifting052_x264: 1530 frames\n",
            "  Class 'Abuse': 10 videos\n",
            "    - Abuse026_x264: 398 frames\n",
            "    - Abuse022_x264: 567 frames\n",
            "    - Abuse001_x264: 546 frames\n",
            "  Class 'RoadAccidents': 30 videos\n",
            "    - RoadAccidents060_x264: 168 frames\n",
            "    - RoadAccidents028_x264: 119 frames\n",
            "    - RoadAccidents005_x264: 45 frames\n",
            "  Class 'Explosion': 10 videos\n",
            "    - Explosion002_x264: 803 frames\n",
            "    - Explosion051_x264: 564 frames\n",
            "    - Explosion016_x264: 193 frames\n",
            "  Class 'Fighting': 10 videos\n",
            "    - Fighting002_x264: 538 frames\n",
            "    - Fighting043_x264: 629 frames\n",
            "    - Fighting011_x264: 1648 frames\n",
            "  Class 'Stealing': 20 videos\n",
            "    - Stealing051_x264: 666 frames\n",
            "    - Stealing088_x264: 2571 frames\n",
            "    - Stealing021_x264: 825 frames\n",
            "  Class 'Arson': 10 videos\n",
            "    - Arson010_x264: 632 frames\n",
            "    - Arson028_x264: 482 frames\n",
            "    - Arson052_x264: 954 frames\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "split_dataset_dir = \"/content/drive/MyDrive/Split_Surveillance_Videos\"\n",
        "train_dir = os.path.join(split_dataset_dir, \"train\")\n",
        "test_dir = os.path.join(split_dataset_dir, \"test\")\n",
        "\n",
        "print(\"\\nTraining Set:\")\n",
        "for class_label in os.listdir(train_dir):\n",
        "    class_train_dir = os.path.join(train_dir, class_label)\n",
        "    if os.path.isdir(class_train_dir):\n",
        "        video_dirs = [d for d in os.listdir(class_train_dir) if os.path.isdir(os.path.join(class_train_dir, d))]\n",
        "        print(f\"  Class '{class_label}': {len(video_dirs)} videos\")\n",
        "\n",
        "        # Optional: Verify frames in a few videos\n",
        "        if video_dirs:\n",
        "            sample_videos = random.sample(video_dirs, min(3, len(video_dirs)))\n",
        "            for video_dir in sample_videos:\n",
        "                video_path = os.path.join(class_train_dir, video_dir)\n",
        "                frames = [f for f in os.listdir(video_path) if f.endswith(\".jpg\")]\n",
        "                print(f\"    - {video_dir}: {len(frames)} frames\")\n",
        "\n",
        "\n",
        "print(\"\\nTesting Set:\")\n",
        "for class_label in os.listdir(test_dir):\n",
        "    class_test_dir = os.path.join(test_dir, class_label)\n",
        "    if os.path.isdir(class_test_dir):\n",
        "        video_dirs = [d for d in os.listdir(class_test_dir) if os.path.isdir(os.path.join(class_test_dir, d))]\n",
        "        print(f\"  Class '{class_label}': {len(video_dirs)} videos\")\n",
        "\n",
        "        if video_dirs:\n",
        "            sample_videos = random.sample(video_dirs, min(3, len(video_dirs)))\n",
        "            for video_dir in sample_videos:\n",
        "                video_path = os.path.join(class_test_dir, video_dir)\n",
        "                frames = [f for f in os.listdir(video_path) if f.endswith(\".jpg\")]\n",
        "                print(f\"    - {video_dir}: {len(frames)} frames\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si2o7ZJW3Ag0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "088b741ad17d4d298620f443dc4ab3cf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1805ac93751945819650fcc45f61cdfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cfd11ca45634afa885264829a1ad937": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4dcf8acc5db04a8096bd4a368d42bafb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_088b741ad17d4d298620f443dc4ab3cf",
            "max": 952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72001a75dd4c4d82a16b9459b9faefef",
            "value": 952
          }
        },
        "4f51c6b93e84462c9854c6406d6d2aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "63c67bfae2974518bebd6c6755bbcefc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9e34e73866f44b397da33f93dbcae07",
              "IPY_MODEL_4dcf8acc5db04a8096bd4a368d42bafb",
              "IPY_MODEL_e352b1f7769845c1a09cbd571a57bfc7"
            ],
            "layout": "IPY_MODEL_3cfd11ca45634afa885264829a1ad937"
          }
        },
        "72001a75dd4c4d82a16b9459b9faefef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9e34e73866f44b397da33f93dbcae07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db46c1870c494be8a2be72510ef4a82d",
            "placeholder": "​",
            "style": "IPY_MODEL_1805ac93751945819650fcc45f61cdfe",
            "value": "Fetching 952 files: 100%"
          }
        },
        "db46c1870c494be8a2be72510ef4a82d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e352b1f7769845c1a09cbd571a57bfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efc87186be6948dd811b8c95eae48e6e",
            "placeholder": "​",
            "style": "IPY_MODEL_4f51c6b93e84462c9854c6406d6d2aa9",
            "value": " 952/952 [00:25&lt;00:00, 63.26it/s]"
          }
        },
        "efc87186be6948dd811b8c95eae48e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
